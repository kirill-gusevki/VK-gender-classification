# -*- coding: utf-8 -*-
"""VK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x5JJRZa8c60z0YUzN0bSSd-53nKPVZU5
"""

import pandas as pd
import numpy as np
from google.colab import drive

import pandas as pd
import numpy as np
from ast import literal_eval

# Путь к данным
DATA_PATH = "/content/"

# 1. Загрузка train_labels
train_labels = pd.read_csv(DATA_PATH + "train_labels.csv", sep=";")
print(f"train_labels: {train_labels.shape}")
print(train_labels.head())

# 2. Загрузка train и test с обработкой user_agent
def parse_user_agent(ua_str):
    try:
        # Исправляем форматирование строки для корректного парсинга
        ua_str = ua_str.replace("'", '"').replace('None', '"None"')
        return literal_eval(ua_str)
    except:
        return {'browser': None, 'browser_version': None, 'os': None, 'os_version': None}

train = pd.read_csv(DATA_PATH + "train.csv", sep=";", on_bad_lines='skip')
test = pd.read_csv(DATA_PATH + "test.csv", sep=";", on_bad_lines='skip')

# Применяем парсер к user_agent
train['user_agent'] = train['user_agent'].apply(parse_user_agent)
test['user_agent'] = test['user_agent'].apply(parse_user_agent)

print(f"\ntrain: {train.shape}")
print(train.head())
print(f"\ntest: {test.shape}")
print(test.head())

# 3. Загрузка referer_vectors
referer_vectors = pd.read_csv(DATA_PATH + "referer_vectors.csv", sep=";")
print(f"\nreferer_vectors: {referer_vectors.shape}")
print(referer_vectors.head())

# 4. Загрузка geo_info
geo_info = pd.read_csv(DATA_PATH + "geo_info.csv", sep=";", dtype=str)
print(f"\ngeo_info: {geo_info.shape}")
print(geo_info.head())

# 5. Загрузка test_users
test_users = pd.read_csv(DATA_PATH + "test_users.csv", sep=";")
print(f"\ntest_users: {test_users.shape}")
print(test_users.head())

# 6. Разделим train_labels на отдельные столбцы
if 'user_id;target' in train_labels.columns:
    train_labels[['user_id', 'target']] = train_labels['user_id;target'].str.split(';', expand=True)
    train_labels = train_labels[['user_id', 'target']]
    train_labels['target'] = train_labels['target'].astype(int)

print("\nОбновленный train_labels:")
print(train_labels.head())

import pandas as pd
import numpy as np
from datetime import datetime

# 1. Улучшенная обработка user_agent
def process_user_agent(df):
    # Развертываем словарь в отдельные колонки
    df_ua = pd.json_normalize(df['user_agent'])
    df = pd.concat([df.drop('user_agent', axis=1), df_ua], axis=1)

    # Гипотеза: больше женщин на iOS
    df['is_ios'] = df['os'].apply(lambda x: 1 if x == 'iOS' else 0)

    # Упрощаем версии ОС (с обработкой ошибок)
    def safe_version_convert(version):
        if pd.isna(version):
            return -1
        parts = str(version).split('.')
        if parts and parts[0].isdigit():
            return int(parts[0])
        return -1

    df['os_version_major'] = df['os_version'].apply(safe_version_convert)

    # Бинаризация популярных браузеров
    browsers = ['Safari', 'Chrome', 'Firefox', 'Edge', 'Opera', 'Yandex Browser']
    for browser in browsers:
        df[f'browser_{browser.lower().replace(" ", "_")}'] = df['browser'].apply(
            lambda x: 1 if x == browser else 0
        )

    return df

# 2. Обработка времени (без изменений)
def process_timestamp(df):
    df['datetime'] = pd.to_datetime(df['request_ts'], unit='s')
    df['hour'] = df['datetime'].dt.hour
    df['day_of_week'] = df['datetime'].dt.dayofweek
    df['is_night'] = df['hour'].apply(lambda x: 1 if x < 6 else 0)
    df['is_morning'] = df['hour'].apply(lambda x: 1 if 6 <= x < 12 else 0)
    df['is_day'] = df['hour'].apply(lambda x: 1 if 12 <= x < 18 else 0)
    df['is_evening'] = df['hour'].apply(lambda x: 1 if x >= 18 else 0)
    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)
    return df.drop('datetime', axis=1)

# 3. Обработка referer (с обработкой ошибок)
def process_referer(df):
    def safe_url_split(url):
        try:
            parts = url.split('/')
            domain = parts[2] if len(parts) > 2 else ''
            path = '/'.join(parts[3:]) if len(parts) > 3 else ''
            return domain, path
        except:
            return '', ''

    df[['referer_domain', 'referer_path']] = df['referer'].apply(
        lambda x: pd.Series(safe_url_split(x)))

    df['url_complexity'] = df['referer_path'].apply(
        lambda x: len(str(x).split('/')) if x else 0)

    return df

# 4. Улучшенная обработка geo_info
def process_geo_info(geo_df):
    # Обработка экспоненциальных значений
    def convert_exp_value(x):
        try:
            if 'E+' in str(x):
                return str(int(float(x)))
            return str(x)
        except:
            return 'unknown'

    geo_df['region_id'] = geo_df['region_id'].apply(convert_exp_value)
    geo_df['region_id'] = geo_df['region_id'].fillna('unknown')
    geo_df['timezone_id'] = geo_df['timezone_id'].fillna('unknown')

    return geo_df

# Применяем обработку
try:
    print("Обработка user_agent...")
    train = process_user_agent(train)
    test = process_user_agent(test)

    print("Обработка timestamp...")
    train = process_timestamp(train)
    test = process_timestamp(test)

    print("Обработка referer...")
    train = process_referer(train)
    test = process_referer(test)

    print("Обработка geo_info...")
    geo_info = process_geo_info(geo_info)

    # 5. Объединение данных с обработкой возможных пропусков
    print("Объединение данных...")
    train = train.merge(geo_info, on='geo_id', how='left')
    test = test.merge(geo_info, on='geo_id', how='left')

    train = train.merge(referer_vectors, on='referer', how='left')
    test = test.merge(referer_vectors, on='referer', how='left')

    # Заполнение пропущенных значений
    for i in range(10):
        comp_col = f'component{i}'
        train[comp_col] = train[comp_col].fillna(train[comp_col].median())
        test[comp_col] = test[comp_col].fillna(test[comp_col].median())

    print("✅ Предобработка завершена успешно!")
    print(f"Train колонки: {train.columns.tolist()}")

except Exception as e:
    print(f"❌ Ошибка при обработке: {e}")
    import traceback
    traceback.print_exc()

# Исправляем только объединение по geo_id
print("Объединение данных...")

# Преобразуем geo_id в geo_info к числовому типу (с обработкой ошибок)
geo_info['geo_id'] = pd.to_numeric(geo_info['geo_id'], errors='coerce')

# Объединяем данные
train = train.merge(geo_info, on='geo_id', how='left')
test = test.merge(geo_info, on='geo_id', how='left')

# Объединяем с векторными представлениями URL
train = train.merge(referer_vectors, on='referer', how='left')
test = test.merge(referer_vectors, on='referer', how='left')

# Заполнение пропущенных значений для компонентов
for i in range(10):
    comp_col = f'component{i}'
    train[comp_col] = train[comp_col].fillna(train[comp_col].median())
    test[comp_col] = test[comp_col].fillna(test[comp_col].median())

print("✅ Объединение данных выполнено успешно!")

def aggregate_user_features(df):
    # Группируем по пользователю
    grouped = df.groupby('user_id')

    # Основные агрегации
    agg_df = grouped.agg({
        # Гипотеза: пользователи iOS чаще женщины
        'is_ios': ['mean', 'sum'],

        # Признаки из user_agent
        'browser': ['nunique', lambda x: x.mode()[0] if not x.mode().empty else 'unknown'],
        'os': [lambda x: x.mode()[0] if not x.mode().empty else 'unknown'],
        'os_version_major': 'mean',

        # Временные признаки
        'hour': ['mean', 'std'],
        'is_night': 'mean',
        'is_morning': 'mean',
        'is_day': 'mean',
        'is_evening': 'mean',
        'is_weekend': 'mean',

        # Гео-признаки
        'country_id': [lambda x: x.mode()[0] if not x.mode().empty else 'unknown'],
        'region_id': ['nunique'],
        'timezone_id': ['nunique'],

        # Признаки из referer
        'referer_domain': ['nunique'],
        'url_complexity': 'mean',

        # Векторные компоненты
        **{f'component{i}': 'mean' for i in range(10)}
    })

    # Упрощение названий колонок
    agg_df.columns = ['_'.join(col).strip() for col in agg_df.columns.values]

    # Переименование ключевых признаков
    agg_df = agg_df.rename(columns={
        'is_ios_mean': 'ios_ratio',
        'is_ios_sum': 'ios_count',
        'browser_<lambda_0>': 'main_browser',
        'os_<lambda_0>': 'main_os',
        'country_id_<lambda_0>': 'main_country',
        'hour_mean': 'avg_hour',
        'hour_std': 'std_hour_activity',
        'is_weekend_mean': 'weekend_ratio'
    })

    # Дополнительные признаки
    agg_df['activity_variability'] = 1 / (1 + agg_df['std_hour_activity'].fillna(0))

    return agg_df.reset_index()

print("Агрегация train...")
train_agg = aggregate_user_features(train)
print("Агрегация test...")
test_agg = aggregate_user_features(test)

print(f"Размер train_agg: {train_agg.shape}, test_agg: {test_agg.shape}")

# Объединяем с целевой переменной
train_final = train_agg.merge(train_labels, on='user_id')

# Проверяем наличие категориальных признаков
categorical_cols = []
for col in ['main_browser', 'main_os', 'main_country']:
    if col in train_final.columns:
        categorical_cols.append(col)

# Обрабатываем только существующие категориальные признаки
if categorical_cols:
    for col in categorical_cols:
        # Объединяем train и test для согласованного кодирования
        combined = pd.concat([train_final[col], test_agg[col]])
        encoded = pd.factorize(combined)[0]
        train_final[col] = encoded[:len(train_final)]
        test_agg[col] = encoded[len(train_final):]

# Заполняем пропуски
for df in [train_final, test_agg]:
    for col in df.columns:
        if df[col].dtype in ['float64', 'int64']:
            df[col].fillna(df[col].median(), inplace=True)
        elif df[col].dtype == 'object':
            df[col].fillna('unknown', inplace=True)

print("✅ Финальные датасеты готовы!")
print(f"Train final: {train_final.shape}, Test agg: {test_agg.shape}")
print(f"Колонки в train_final: {train_final.columns.tolist()}")

# Переименовываем столбцы для удобства
train_final = train_final.rename(columns={
    'os_<lambda>': 'main_os',
    'country_id_<lambda>': 'main_country'
})

test_agg = test_agg.rename(columns={
    'os_<lambda>': 'main_os',
    'country_id_<lambda>': 'main_country'
})

# Преобразуем категориальные признаки в числовые коды
categorical_cols = ['main_os', 'main_country', 'main_browser']

for col in categorical_cols:
    # Объединяем train и test для согласованного кодирования
    combined = pd.concat([train_final[col], test_agg[col]])
    encoded = pd.factorize(combined)[0]

    # Разделяем обратно на train и test
    train_final[col] = encoded[:len(train_final)]
    test_agg[col] = encoded[len(train_final):]

# Убедимся, что все типы данных числовые
print("\nПроверка типов данных после преобразования:")
print(train_final.dtypes.value_counts())

import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Подготовка данных
X = train_final.drop(['user_id', 'target'], axis=1)
y = train_final['target']

# Разделение на тренировочную и валидационную выборки
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Создание датасетов LightGBM
train_data = lgb.Dataset(X_train, label=y_train)
val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

# Параметры модели
params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'num_leaves': 63,
    'learning_rate': 0.05,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': -1,
    'seed': 42,
    'max_depth': 7,
    'min_data_in_leaf': 100
}

# Обучение модели
model = lgb.train(params,
                 train_data,
                 num_boost_round=1000,
                 valid_sets=[val_data],
                 callbacks=[
                     lgb.early_stopping(stopping_rounds=50, verbose=True),
                     lgb.log_evaluation(period=50)
                 ])

# Прогнозирование на валидации
val_pred = model.predict(X_val)
auc_score = roc_auc_score(y_val, val_pred)
print(f"\n✅ AUC на валидации: {auc_score:.4f}")

# Важность признаков
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.feature_importance()
}).sort_values('Importance', ascending=False)

print("\nТоп-15 важных признаков:")
print(feature_importance.head(15))

# Прогнозирование для тестовых данных
test_pred = model.predict(test_agg.drop('user_id', axis=1))
test_agg['predicted_target'] = test_pred

# Формирование финального результата
submission = test_agg[['user_id', 'predicted_target']].rename(columns={'predicted_target': 'target'})
submission['target'] = (submission['target'] > 0.5).astype(int)

# Сохранение результатов
submission.to_csv('gender_prediction_submission.csv', index=False)
print("\n✅ Результаты сохранены в gender_prediction_submission.csv")

import joblib
from lightgbm import LGBMClassifier

# 1. Пересоздадим модель с лучшими параметрами на всех данных
# (используем параметры из лучшей итерации)
best_params = model.params
final_model = LGBMClassifier(**best_params)
final_model.fit(X, y)  # Обучаем на всех данных

# 2. Сохраняем модель в формате .joblib
model_filename = 'gender_prediction_model.joblib'
joblib.dump(final_model, model_filename)
print(f"✅ Модель сохранена как {model_filename}")

# 3. Сохраняем список признаков (важно для воспроизводимости)
import json
feature_names = list(X.columns)
with open('feature_names.json', 'w') as f:
    json.dump(feature_names, f)
print(f"✅ Список признаков сохранен как feature_names.json")

!zip -r vk_gender_prediction_solution.zip \
    gender_prediction_model.joblib \
    feature_names.json \
    gender_prediction_submission.csv \
    VK.ipynb